[global]
seed = 37

[data]
# variables_csv = "data/Variables.csv"
variables_csv = "data/Variables_with_keywords_and_desc.csv"
keywords_csv = "data/Keywords_summarized_new.csv"

[fields]
# Configure which columns feed embeddings and metadata for variable rows.
embedding_columns = ["label", "name", "description"]
metadata_columns = ["label", "name", "dataset", "description"]
gold_labels_column = "keywords"
dataset_column = "dataset"
# Optional chunking controls for long text fields. Set ``embedding_chunk_chars`` to
# split fields into overlapping slices instead of truncating them, and tune
# ``chunk_overlap`` to retain context between slices.
# embedding_chunk_chars = 512
# chunk_overlap = 64

[taxonomy_fields]
# Map logical taxonomy fields to the column names used in your keywords CSV.
# Set a field to "" or omit it if the column is unavailable.
# name = "name"
# parent = "parent"
# parents = "parents" # optional: pipe-delimited multi-parent relationships
# order = "order"
# definition = "definition"
definition = "definition_summary" # use this when working with summarized definitions
# label = "label"

[embedder]
# model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
# model_name = "BAAI/bge-large-en-v1.5"
models = ["BAAI/bge-large-en-v1.5", "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"]
max_length = 512
batch_size = 256
fp16 = true
# device = "cpu"
mean_pool = false # false = CLS token
pca_components = 256 # set to project concatenated embeddings (if models is provided)
pca_whiten = true

[taxonomy_embeddings]
gamma = 0.05
summary_weight = 0.85
# Pull child signal back into ancestors; set to 0.0 to disable.
child_aggregation_weight = 0.1
# Limit how many ancestor hops receive child signal (unset = to the root).
# child_aggregation_depth = 1

[hnsw]
space = "cosine"
M = 32
ef_construction = 200
ef_search = 128
num_threads = 0

[evaluation]
n = 10_000
seed = 37
# Provide column names as they appear in your variables CSV.
dedupe_on = ["name"]
progress_log_interval = 10

[pruning]
enable_taxonomy_pruning = true

tree_sort_mode = "relevance" # options: relevance, topological, alphabetical, proximity, pagerank
suggestion_sort_mode = "relevance" # independent ranking for the suggestion list; options mirror tree_sort_mode
suggestion_list_limit = 6 # number of candidates surfaced alongside the tree

 # options: dominant_forest, anchor_hull, similarity_threshold, radius, steiner_similarity, community_pagerank
pruning_mode = "anchor_hull"

# similarity_threshold = 0.0 # cosine threshold used when pruning_mode="similarity_threshold"
# pruning_radius = 2 # undirected hop limit when pruning_mode="radius"

anchor_top_k = 8 # ANN anchors to fetch per item before pruning
max_descendant_depth = 4 # limit descendants pulled under each anchor
lexical_anchor_limit = 3 # additional anchors sourced from lexical overlap
community_clique_size = 2 # k for k-clique community expansion
max_community_size = 160 # max nodes pulled from any single community, if set

anchor_overfetch_multiplier = 6 # ANN search overfetch multiplier (before pruning)
anchor_min_overfetch = 96

pagerank_damping = 0.875
pagerank_score_floor = 0.0
# Cap candidate nodes before PageRank; the smaller of this and node_budget wins.
# pagerank_candidate_limit = 384

node_budget = 200 # hard cap on nodes retained in the final allowed set
# surrogate_root_label = "Study catalogue variables" # optional: introduce a surrogate root label for the taxonomy tree

[llm]
endpoint = "http://127.0.0.1:8080/v1"
model = ""
api_key = "" # optional; leave blank to use OPENAI_API_KEY or the default local key
n_predict = 32
temperature = 0.0
top_k = 20
top_p = 0.8
min_p = 0.0
cache_prompt = true
n_keep = -1
force_slot_id = false # set true to force explicit slot ids on the llama server
embedding_remap_threshold = 0.25
alias_similarity_threshold = 0.9
# Enable snapping ancestor predictions down to a better-matching child label.
snap_to_child = false
snap_margin = 0.25
# Options: "token_sort", "token_set", "embedding"
snap_similarity = "token_set"
# Maximum descendant depth to consider when snapping (1 = direct children only).
snap_descendant_depth = 2

[prompts]
system_template_path = "templates/match_system_prompt.j2"
user_template_path = "templates/match_user_prompt.j2"

[parallelism]
num_slots = 4
pool_maxsize = 8
pruning_workers = 8
pruning_batch_size = 8
pruning_queue_size = 16
# Pruning precomputes embeddings on the coordinator and fans out work through a
# thread pool by default. Enable ``pruning_embed_on_workers`` to spin up a
# process pool that clones the embedder inside each worker.
pruning_embed_on_workers = false
# When cloning the embedder provide explicit device mappings if GPUs are
# available.
# pruning_worker_devices = ["cuda:0", "cuda:1"]
# pruning_start_method = "auto"  # or "fork"/"spawn" to override process startup

[http]
sock_connect = 10.0
sock_read_floor = 30.0
