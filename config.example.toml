[data]
variables_csv = "data/Variables.csv"
keywords_csv = "data/Keywords_summarized.csv"

[embedder]
model_name = "cambridgeltl/SapBERT-from-PubMedBERT-fulltext"
# model_name = "BAAI/bge-large-en"
max_length = 512
batch_size = 256
fp16 = true
mean_pool = false # false = CLS token

[taxonomy_embeddings]
gamma = 0.125
summary_weight = 0.5

[hnsw]
space = "cosine"
M = 32
ef_construction = 200
ef_search = 128
num_threads = 0

[evaluation]
endpoint = "http://127.0.0.1:8080/completions"
n = 10_000
seed = 37
dedupe_on = ["name"]
# Tree pruning
anchor_top_k = 36 # ANN anchors to fetch per item before pruning
max_descendant_depth = 5 # limit descendants pulled under each anchor
suggestion_list_limit = 8 # number of candidates surfaced alongside the tree
lexical_anchor_limit = 3 # additional anchors sourced from lexical overlap
community_clique_size = 2 # k for k-clique community expansion
max_community_size = 256 # max nodes pulled from any single community, if set
anchor_overfetch_multiplier = 3 # ANN search overfetch multiplier (before pruning)
anchor_min_overfetch = 128
pagerank_damping = 0.85
pagerank_score_floor = 0.0
# Cap candidate nodes before PageRank; the smaller of this and node_budget wins.
pagerank_candidate_limit = 512
node_budget = 640 # hard cap on nodes retained in the final allowed set
# LLM tunables
n_predict = 32
temperature = 0.0
llm_top_k = 20
llm_top_p = 0.8
llm_min_p = 0.0
llm_cache_prompt = true
llm_n_keep = -1
# Parallelism tunables
num_slots = 6
pool_maxsize = 16
max_workers = 6
http_sock_connect = 10.0
http_sock_read_floor = 30.0
progress_log_interval = 10
